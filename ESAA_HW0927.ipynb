{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO2rhMECvlCExGqOnH96dH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whyeon92/ESAA_OB/blob/Code_Study/ESAA_HW0927.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 텍스트 분석 이해  "
      ],
      "metadata": {
        "id": "smFlsUQQfVvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[진행 프로세스]**\n",
        " 1. 텍스트 사전 준비 작업, 전처리  \n",
        "   : 텍스트를 피처로 만들기 전 클렌징, 대/소문자 변경, 특수문자 삭제, 단어 토큰화, 의미 없는 단어(Stop Word) 제거, 어근 추출 등의 텍스트 정규화 작업\n",
        " 2. 피처/벡터화 추출  \n",
        "   : 가공된 텍스트에서 피처 추출, 해당 피처에 벡터 값 할당   \n",
        "    → BOW, Word2Vec 이용\n",
        " 3. ML 모델 수립 및 학습/예측/평가  \n",
        "   : 피처 벡터화된 데이터 세트에 머신러닝 모델 적용"
      ],
      "metadata": {
        "id": "pDZzK7ypfbiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
      ],
      "metadata": {
        "id": "QGM5kLNiEmWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화"
      ],
      "metadata": {
        "id": "NTeLNOqKIFSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29IyyIcPEhFf",
        "outputId": "9d77647c-4b40-40d4-ea3e-ea103305c25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화"
      ],
      "metadata": {
        "id": "-wytSbqFIG92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eopP-Fg1G6MU",
        "outputId": "ab3204b8-3e12-49a4-e6b7-bcfd95d69521"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "\n",
        "    #문장별로 분리 토큰\n",
        "    sentences = sent_tokenize(text)\n",
        "    #분리된 문장별 단어 토큰화\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens\n",
        "\n",
        "#여러 문장들에 대해 문장별 단어 토큰화 수행\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens),len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGl1Kt15HABH",
        "outputId": "dd0040ae-56e3-42c6-8129-59d12f91e83f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "스탑워드 제거"
      ],
      "metadata": {
        "id": "AlQPldzUIB03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8rWd_SjHJjB",
        "outputId": "94b89346-daf1-457a-8abe-cdaa77295314"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 개수:',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZxIe_7mHKOV",
        "outputId": "76e83a45-c59b-4167-9748-dd8c8ec9fbc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 개수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "#위 예제에서 3개의 문장별로 얻은 word_tokens list 에 대해 스톱 워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "    filtered_words=[]\n",
        "    #개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "    for word in sentence:\n",
        "        #소문자로 모두 변환\n",
        "        word = word.lower()\n",
        "        #토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg6xuXHwHM1P",
        "outputId": "283b19f6-4ca4-4823-a143-86712c177720"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming과 Lemmatization"
      ],
      "metadata": {
        "id": "mjiyxChhIz3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqeMy3HGHSPO",
        "outputId": "fc84e635-90d0-407b-a0d3-afc3cedbecf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBwe1E_0HSNL",
        "outputId": "51b3eae7-7e4c-43b2-cb5a-8c68229f512c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[클렌징]**  \n",
        " 텍스트에서 분석에 오히려 방해가 되는 불필요한 문자들을 사전에 제거\n",
        "\n",
        "**[토큰화]**  \n",
        " 텍스트를 단위에 따라 나누는 작업.\n",
        " - 문장 토큰화 : 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호로 문장을 나누기\n",
        " - 단어 토큰화 : 문장을 단어로 토큰화. 기본적으로 공백을 단위로 단어를 분리  \n",
        "\n",
        "⚠️ 문장을 단어별로 하나씩 토큰화할 경우 문맥적 의미가 사라짐!  \n",
        "→ n-gram으로 해결  \n",
        " n-gram: 단어를 하나하나 토큰화하는 게 아닌 n개의 단어를 하나의 토큰으로 생각하여 토큰화!\n",
        "\n",
        "**[스톱워드 제거]**  \n",
        " 스톱워드: 분석에 큰 의미를 갖지 않는 be동사, 관사 등의 단어  \n",
        " 즉, 필수 문법적 요소지만 문맥적으로 큰 의미가 없는 단어  \n",
        " 오히려 중요한 단어로 인지되면 안되는데 자주 나와서 그렇게 해석될 수 있기에 제거 필수  \n",
        " → nltk의 스톱워드 목록에서 필터링!\n",
        "\n",
        "**[Stemming / Lemmatization]**  \n",
        " 둘 다 단어의 원형인 어근을 매핑  \n",
        "- Stemming  \n",
        " : 일반적인 방법, 더 단순화된 방법을 사용해서 철자가 훼손된 경우가 잦음!\n",
        "- Lemmatization\n",
        " : 문법적 요소와 더 의미적인 부분을 찾아줘서 오랜시간이 걸리지만 더 깔끔한 처리"
      ],
      "metadata": {
        "id": "ykj79fGoenHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.3 Bag of Words - BOW"
      ],
      "metadata": {
        "id": "LDHh26WDJA3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dense = np.array([[3, 0, 1], [0, 2, 0 ]])"
      ],
      "metadata": {
        "id": "2QyMJSStHSJC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "#0이 아닌 데이터 추출\n",
        "data = np.array([3,1,2])\n",
        "\n",
        "#행 위치와 열 위치를 각각 배열로 생성\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "\n",
        "#sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
      ],
      "metadata": {
        "id": "MVvYhGIrHSG5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkSyd9FWHSAz",
        "outputId": "79288540-6eec-44b4-bea9-a4f5b88a9172"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "                    [1,4,0,3,2,5],\n",
        "                    [0,6,0,3,0,0],\n",
        "                    [2,0,0,0,0,0],\n",
        "                    [0,0,0,7,0,8],\n",
        "                    [1,0,0,0,0,0]])\n",
        "\n",
        "#0이 아닌 데이터 추출\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "\n",
        "#행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "\n",
        "#COO 형식으로 변환\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
        "\n",
        "#행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "#CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j0mB7bHHR-2",
        "outputId": "e98bcc0c-c421-47c4-d47e-ac54b0b07ad2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "             [1,4,0,3,2,5],\n",
        "             [0,6,0,3,0,0],\n",
        "             [2,0,0,0,0,0],\n",
        "             [0,0,0,7,0,8],\n",
        "             [1,0,0,0,0,0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "metadata": {
        "id": "Hs9KpXxKHR8o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Bag of Words]**  \n",
        ": 문서가 가지는 모든 단어를 문맥, 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델  \n",
        "\n",
        " - 🥰쉽고 빠르게 구축 가능, 그냥 간단히 단어 토큰화하고 횟수 세기\n",
        " - 🥰활용도가 높음\n",
        "\n",
        " - 🤢문맥 의미를 너무 반영 못함\n",
        " - 🤢희소 행렬 문제  \n",
        "    : 문서마다 서로 다른 단어로 구성되어서 BOW의 단어 중 해당 문서에 사용되지 않는 단어가 더 많을 수 있음. 그런 경우 행렬이 대부분의 값이 0으로 채워지는 희소행렬이 되는데, 이는 일반적으로 머신러닝 알고리즘의 수행시간과 예측성능을 떨어뜨리기 때문에 나쁘다!  \n",
        "\n",
        "**[벡터 피처화]**  \n",
        " BOW에서 벡터 피처화 → 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 단어의 횟수(또는 정규화된 빈도)를 값으로 부여하는 데이터 세트 모델로 변경 → M개의 텍스트문서와 N개의 단어 → M*N형태의 행렬 구성  \n",
        " - 카운트 기반 벡터화  \n",
        "  → 카운트 횟수가 많으면 중요하다! 그러나 이럴 경우 문서의 특징을 나타내기 보단 언어의 특성상 문장에 자주 사용될 수 밖에 없는 단어도 높은 값이 부여될 가능성이 존재.\n",
        " - TF-IDF 기반 벡터화  \n",
        "   → 개별 문서에서 자주 나타나는 단어에 높은 가중치, 그러나 모든 문서에 자주 나타나는 단어엔 페널티를 줘서 위의 카운트 기반에서 나타나는 단점을 어느정도 완화."
      ],
      "metadata": {
        "id": "is7P88DYa9mP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAcXYxBMIrGU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}